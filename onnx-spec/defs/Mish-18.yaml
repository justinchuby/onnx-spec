domain: ''
name: Mish
since_version: 18
min_input: 1
max_input: 1
min_output: 1
max_output: 1
doc: '

  Mish: A Self Regularized Non-Monotonic Neural Activation Function.


  Perform the linear unit element-wise on the input tensor X using formula:


  ```

  mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))

  ```

  '
attributes: []
inputs:
- name: X
  type_str: T
  description: Input tensor
  min_arity: 1
  tags:
  - differentiable
outputs:
- name: Y
  type_str: T
  description: Output tensor
  min_arity: 1
  tags:
  - differentiable
type_constraints:
- type_param_str: T
  description: Constrain input X and output types to float tensors.
  allowed_type_strs:
  - tensor(float16)
  - tensor(float)
  - tensor(double)
support_level: COMMON
deprecated: false
