attributes:
- default_value: 0.0
  description: The decay factor of learning rate after one update.The effective learning
    rate is computed by r = R / (1 + T * decay_factor). Default to 0 so that increasing
    update counts doesn't reduce the learning rate.
  name: decay_factor
  required: false
  type: AttrType.FLOAT
- default_value: 9.999999974752427e-07
  description: Small scalar to avoid dividing by zero.
  name: epsilon
  required: false
  type: AttrType.FLOAT
- default_value: 0.0
  description: Regularization coefficient in 0.5 * norm_coefficient * ||X||_2^2. Default
    to 0, which means no regularization.
  name: norm_coefficient
  required: false
  type: AttrType.FLOAT
deprecated: false
doc: "\n    Compute one iteration of ADAGRAD, a stochastic gradient based optimization\n\
  \    algorithm. This operator can conduct the optimization of multiple tensor variables.\n\
  \n    Let's define the behavior of this operator. As you can imagine, ADAGRAD requires\n\
  \    some parameters:\n\n     - The initial learning-rate \"R\".\n     - The update\
  \ count \"T\". That is, the number of training iterations conducted.\n     - A L2-norm\
  \ regularization coefficient \"norm_coefficient\".\n     - A learning-rate decay\
  \ factor \"decay_factor\".\n     - A small constant \"epsilon\" to avoid dividing-by-zero.\n\
  \n    At each ADAGRAD iteration, the optimized tensors are moved along a direction\n\
  \    computed based on their estimated gradient and accumulated squared gradient.\
  \ Assume\n    that only a single tensor \"X\" is updated by this operator. We need\
  \ the value of \"X\",\n    its gradient \"G\", and its accumulated squared gradient\
  \ \"H\". Therefore, variables in\n    this operator's input list are sequentially\
  \ \"R\", \"T\", \"X\", \"G\", and \"H\". Other\n    parameters are given as attributes\
  \ because they are usually constants. Also, the\n    corresponding output tensors\
  \ are the new value of \"X\" (called \"X_new\"), and then\n    the new accumulated\
  \ squared gradient (called \"H_new\"). Those outputs are computed\n    from the\
  \ given inputs following the pseudo code below.\n\n    Let \"+\", \"-\", \"*\",\
  \ and \"/\" are all element-wise arithmetic operations with\n    numpy-style broadcasting\
  \ support. The pseudo code to compute those outputs is:\n\n      // Compute a scalar\
  \ learning-rate factor. At the first update of X, T is generally\n      // 0 (0-based\
  \ update index) or 1 (1-based update index).\n      r = R / (1 + T * decay_factor);\n\
  \n      // Add gradient of 0.5 * norm_coefficient * ||X||_2^2, where ||X||_2 is\
  \ the 2-norm.\n      G_regularized = norm_coefficient * X + G;\n\n      // Compute\
  \ new accumulated squared gradient.\n      H_new = H + G_regularized * G_regularized;\n\
  \n      // Compute the adaptive part of per-coordinate learning rate. Note that\
  \ Sqrt(...)\n      // computes element-wise square-root.\n      H_adaptive = Sqrt(H_new)\
  \ + epsilon\n\n      // Compute the new value of \"X\".\n      X_new = X - r * G_regularized\
  \ / H_adaptive;\n\n    If one assign this operators to optimize multiple inputs,\
  \ for example, \"X_1\" and \"X_2\", the same\n    pseudo code may be extended to\
  \ handle all tensors jointly. More specifically, we can view \"X\" as a\n    concatenation\
  \ of \"X_1\" and \"X_2\" (of course, their gradient and accumulate gradient should\n\
  \    be concatenated too) and then just reuse the entire pseudo code.\n\n    Note\
  \ that ADAGRAD was first proposed in http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf.\n\
  \    In that reference paper, this operator is a special case of the Figure 1's\
  \ composite mirror\n    descent update.\n"
domain: ai.onnx.preview.training
inputs:
- description: The initial learning rate.
  min_arity: 1
  name: R
  tags: []
  type_str: T1
- description: The update count of "X". It should be a scalar.
  min_arity: 1
  name: T
  tags: []
  type_str: T2
- description: The current values of optimized tensors, followed by their respective
    gradients, followed by their respective accumulated squared gradients.For example,
    if two tensor "X_1" and "X_2" are optimized, The input list would be ["X_1", "X_2",
    gradient of "X_1", gradient of "X_2", accumulated squared gradient of "X_1", accumulated
    squared gradient of "X_2"].
  min_arity: 1
  name: inputs
  tags:
  - variadic
  - heterogeneous
  type_str: T3
max_input: 2147483647
max_output: 2147483647
min_input: 3
min_output: 1
name: Adagrad
outputs:
- description: Updated values of optimized tensors, followed by their updated values
    of accumulated squared gradients. For example, if two tensor "X_1" and "X_2" are
    optimized, the output list would be [new value of "X_1," new value of "X_2" new
    accumulated squared gradient of "X_1", new accumulated squared gradient of "X_2"].
  min_arity: 1
  name: outputs
  tags:
  - variadic
  - heterogeneous
  type_str: T3
since_version: 1
support_level: SupportType.COMMON
type_constraints:
- allowed_type_strs:
  - tensor(float)
  - tensor(double)
  description: Constrain input types to float scalars.
  type_param_str: T1
- allowed_type_strs:
  - tensor(int64)
  description: Constrain input types to 64-bit integer scalars.
  type_param_str: T2
- allowed_type_strs:
  - tensor(float)
  - tensor(double)
  description: Constrain input and output types to float tensors.
  type_param_str: T3
