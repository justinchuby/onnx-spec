domain: ai.onnx.preview.training
name: Adagrad
since_version: 1
min_input: 3
max_input: 2147483647
min_output: 1
max_output: 2147483647
doc: "\nCompute one iteration of ADAGRAD, a stochastic gradient based optimization\n\
  algorithm. This operator can conduct the optimization of multiple tensor variables.\n\
  \nLet's define the behavior of this operator. As you can imagine, ADAGRAD requires\n\
  some parameters:\n\n - The initial learning-rate \"R\".\n - The update count \"\
  T\". That is, the number of training iterations conducted.\n - A L2-norm regularization\
  \ coefficient \"norm_coefficient\".\n - A learning-rate decay factor \"decay_factor\"\
  .\n - A small constant \"epsilon\" to avoid dividing-by-zero.\n\nAt each ADAGRAD\
  \ iteration, the optimized tensors are moved along a direction\ncomputed based on\
  \ their estimated gradient and accumulated squared gradient. Assume\nthat only a\
  \ single tensor \"X\" is updated by this operator. We need the value of \"X\",\n\
  its gradient \"G\", and its accumulated squared gradient \"H\". Therefore, variables\
  \ in\nthis operator's input list are sequentially \"R\", \"T\", \"X\", \"G\", and\
  \ \"H\". Other\nparameters are given as attributes because they are usually constants.\
  \ Also, the\ncorresponding output tensors are the new value of \"X\" (called \"\
  X_new\"), and then\nthe new accumulated squared gradient (called \"H_new\"). Those\
  \ outputs are computed\nfrom the given inputs following the pseudo code below.\n\
  \nLet \"+\", \"-\", \"*\", and \"/\" are all element-wise arithmetic operations\
  \ with\nnumpy-style broadcasting support. The pseudo code to compute those outputs\
  \ is:\n\n  // Compute a scalar learning-rate factor. At the first update of X, T\
  \ is generally\n  // 0 (0-based update index) or 1 (1-based update index).\n  r\
  \ = R / (1 + T * decay_factor);\n\n  // Add gradient of 0.5 * norm_coefficient *\
  \ ||X||_2^2, where ||X||_2 is the 2-norm.\n  G_regularized = norm_coefficient *\
  \ X + G;\n\n  // Compute new accumulated squared gradient.\n  H_new = H + G_regularized\
  \ * G_regularized;\n\n  // Compute the adaptive part of per-coordinate learning\
  \ rate. Note that Sqrt(...)\n  // computes element-wise square-root.\n  H_adaptive\
  \ = Sqrt(H_new) + epsilon\n\n  // Compute the new value of \"X\".\n  X_new = X -\
  \ r * G_regularized / H_adaptive;\n\nIf one assign this operators to optimize multiple\
  \ inputs, for example, \"X_1\" and \"X_2\", the same\npseudo code may be extended\
  \ to handle all tensors jointly. More specifically, we can view \"X\" as a\nconcatenation\
  \ of \"X_1\" and \"X_2\" (of course, their gradient and accumulate gradient should\n\
  be concatenated too) and then just reuse the entire pseudo code.\n\nNote that ADAGRAD\
  \ was first proposed in http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf.\n\
  In that reference paper, this operator is a special case of the Figure 1's composite\
  \ mirror\ndescent update.\n"
attributes:
- name: decay_factor
  description: The decay factor of learning rate after one update.The effective learning
    rate is computed by r = R / (1 + T * decay_factor). Default to 0 so that increasing
    update counts doesn't reduce the learning rate.
  type: AttrType.FLOAT
  required: false
  default_value: 0.0
- name: epsilon
  description: Small scalar to avoid dividing by zero.
  type: AttrType.FLOAT
  required: false
  default_value: 9.999999974752427e-07
- name: norm_coefficient
  description: Regularization coefficient in 0.5 * norm_coefficient * ||X||_2^2. Default
    to 0, which means no regularization.
  type: AttrType.FLOAT
  required: false
  default_value: 0.0
inputs:
- name: R
  type_str: T1
  description: The initial learning rate.
  min_arity: 1
  tags: []
- name: T
  type_str: T2
  description: The update count of "X". It should be a scalar.
  min_arity: 1
  tags: []
- name: inputs
  type_str: T3
  description: The current values of optimized tensors, followed by their respective
    gradients, followed by their respective accumulated squared gradients.For example,
    if two tensor "X_1" and "X_2" are optimized, The input list would be ["X_1", "X_2",
    gradient of "X_1", gradient of "X_2", accumulated squared gradient of "X_1", accumulated
    squared gradient of "X_2"].
  min_arity: 1
  tags:
  - variadic
  - heterogeneous
outputs:
- name: outputs
  type_str: T3
  description: Updated values of optimized tensors, followed by their updated values
    of accumulated squared gradients. For example, if two tensor "X_1" and "X_2" are
    optimized, the output list would be [new value of "X_1," new value of "X_2" new
    accumulated squared gradient of "X_1", new accumulated squared gradient of "X_2"].
  min_arity: 1
  tags:
  - variadic
  - heterogeneous
type_constraints:
- type_param_str: T1
  description: Constrain input types to float scalars.
  allowed_type_strs:
  - tensor(float)
  - tensor(double)
- type_param_str: T2
  description: Constrain input types to 64-bit integer scalars.
  allowed_type_strs:
  - tensor(int64)
- type_param_str: T3
  description: Constrain input and output types to float tensors.
  allowed_type_strs:
  - tensor(float)
  - tensor(double)
support_level: COMMON
deprecated: false
