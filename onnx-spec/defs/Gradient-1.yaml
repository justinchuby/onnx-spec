attributes:
- default_value: null
  description: Input tensor names of the differentiated sub-graph. It contains only
    the necessary differentiated inputs of a (sub-)graph. Variables (usually called
    intermediate variables) that can be generated from inputs cannot be included in
    this attribute.
  name: xs
  required: true
  type: AttrType.STRINGS
- default_value: null
  description: The targeted tensor. It can be viewed as the output of the differentiated
    function. The attribute "xs" and attribute "zs" are the minimal independent variable
    set that determines the value of "y".
  name: y
  required: true
  type: AttrType.STRING
- default_value: null
  description: Input tensor names of the differentiated sub-graph. It contains only
    the necessary non-differentiated inputs of a (sub-)graph. Variables (usually called
    intermediate variables) that can be generated from inputs cannot be included in
    this attribute.
  name: zs
  required: false
  type: AttrType.STRINGS
deprecated: false
doc: "\nGradient operator computes the partial derivatives of a specific tensor w.r.t.\n\
  some other tensors. This operator is widely used in gradient-based training\nalgorithms.\
  \ To illustrate its use, let's consider a computation graph,\n\n```\nX -----.\n\
  \       |\n       v\nW --> Conv --> H --> Gemm --> Y\n                      ^\n\
  \                      |\n                      Z\n```\n\n, where W and Z are trainable\
  \ tensors. Note that operators' attributes are\nomitted for the sake of simplicity.\
  \ Let dY/dW (dY/dZ) be the gradient of\nY with respect to W (Z). The user can compute\
  \ gradient by inserting Gradient\noperator to form another graph shown below.\n\n\
  ```\nW --> Conv --> H --> Gemm --> Y\n|      ^              ^\n|      |        \
  \      |\n|      X              Z\n|      |              |\n|      |   .----------'\n\
  |      |   |  (W/Z/X is the 1st/2nd/3rd input of Gradient as shown in\n|      |\
  \   |   \"xs\" followed by \"zs\")\n|      v   v\n'---> Gradient(xs=[\"W\", \"Z\"\
  ], zs=[\"X\"], y=\"Y\")\n       |   |\n       |   '----------------------------------->\
  \ dY/dW (1st output of Gradient)\n       |\n       '--------------------------------------->\
  \ dY/dZ (2nd output of Gradient)\n```\n\nBy definition, the tensor \"y\" is a function\
  \ of independent variables in \"xs\"\nand \"zs\". Since we only compute the gradient\
  \ of \"y\" w.r.t. the differentiable\nvariables in \"xs\", this Gradient only outputs\
  \ dY/dW and dY/dZ. Note that \"H\"\ncannot appear in \"xs\" and \"zs\". The reason\
  \ is that \"H\" can be determined by\ntensors \"W\" and \"X\" and therefore \"H\"\
  \ is not an independent variable.\n\nAll outputs are optional. If needed, for example,\
  \ user can assign an empty\nstring to the 1st output name of that Gradient to skip\
  \ the generation of dY/dW.\nNote that the concept of optional outputs can also be\
  \ found in ONNX's RNN, GRU,\nand LSTM.\n\nGradient operator can compute derivative\
  \ against intermediate tensors. For\nexample, the gradient of Y with respect to\
  \ H can be done via\n\n```\nW --> Conv --> H --> Gemm --> Y\n       ^       |  \
  \    ^\n       |       |      |\n       X       |      Z\n       .-------'     \
  \ |\n       |   .----------'\n       |   | (H/Z is the 1st/2nd input of Gradient\
  \ as shown in \"xs\")\n       v   v\n      Gradient(xs=[\"H\", \"Z\"], y=\"Y\")\n\
  \       |   |\n       |   '-----------------------------------> dY/dH (1st output\
  \ of Gradient)\n       |\n       '---------------------------------------> dY/dZ\
  \ (2nd output of Gradient)\n```\n\nIt is possible to represent high-order differentiation\
  \ using Gradient operators.\nFor example, given the following linear model:\n\n\
  ```\nW --> Gemm --> Y --> Loss --> O\n       ^              ^\n       |        \
  \      |\n       X              L\n```\n\nTo compute the 2nd order derivative of\
  \ O with respect to W (denoted by\nd^2O/dW^2), one can do\n\n```\nW --> Gemm -->\
  \ Y --> Loss --> O\n|      ^              ^\n|      |              |\n|      X .------------L\n\
  |      | |            |\n|      | |            v\n+------+-+> Gradient(xs=[\"X\"\
  , \"W\"], zs=[\"L\"], y=\"O\") ---> dO/dX (1st output of Gradient)\n|      | | \
  \   |\n|      | |    '---> dO/dW (2nd output of Gradient)\n|      v v\n'---> Gradient(xs=[\"\
  X\", \"W\"], zs=[\"L\"], y=\"dO/dW\") ---> d(dO/dW)dX (1st output of\n       | \
  \                                                 Gradient)\n       |\n       |\n\
  \       '---> d^2O/dW^2 (2nd output of Gradient)\n```\n\nThe tensors named in attributes\
  \ \"xs\", \"zs\", and \"y\" define the differentiated\ncomputation graph, and the\
  \ inputs to Gradient node define the values at\nwhich the gradient is computed.\
  \ We can feed different tensors to the identified\ngraph. For example, one can compute\
  \ the gradient of Y with respect to H at\na specific value of H, H_1, by providing\
  \ that value as an input to the Gradient\nnode.\n\n```\nW --> Conv --> H --> Gemm\
  \ --> Y\n       ^              ^\n       |              |\n       X            \
  \  Z\n\n          Z_1 (2nd input of Gradient)\n           |\n           v\nH_1 -->\
  \ Gradient(xs=[\"H\", \"Z\"], y=\"Y\") ---> dY/dH when H = H_1 and Y = Y_1.\n  \
  \         |\n           '------------------------------> dY/dZ (2nd output of Gradient)\n\
  ```\n\nWhen the inputs of Gradient are the tensors named in \"xs\" and \"zs\", the\n\
  computation can be optimized. More specifically, intermediate variables in\nforward\
  \ pass can be reused if the gradient is computed via reverse-mode\nauto-differentiation.\n\
  \n"
domain: ai.onnx.preview.training
inputs:
- description: The values fed into graph identified by the attributes. The i-th input
    is the value of the i-th tensor specified in the concatenated list of the attribute
    "xs" and the attribute  "zs". For example, if xs=["A", "B"] and zs=["C"], the
    first input is used as the value of symbol "A" and the 3rd input is substituted
    for all the occurrences of "C".
  min_arity: 1
  name: Inputs
  tags:
  - variadic
  - heterogeneous
  type_str: T1
max_input: 2147483647
max_output: 2147483647
min_input: 1
min_output: 1
name: Gradient
outputs:
- description: The gradient of the tensor specified by the attribute "y" with respect
    to each of tensors specified in the attribute "xs". The i-th output is the gradient
    of "y" with respect to the i-th tensor specified in the attribute "xs".
  min_arity: 1
  name: Outputs
  tags:
  - variadic
  - heterogeneous
  type_str: T2
since_version: 1
support_level: SupportType.COMMON
type_constraints:
- allowed_type_strs:
  - tensor(uint8)
  - tensor(uint16)
  - tensor(uint32)
  - tensor(uint64)
  - tensor(int8)
  - tensor(int16)
  - tensor(int32)
  - tensor(int64)
  - tensor(float16)
  - tensor(float)
  - tensor(double)
  - tensor(string)
  - tensor(bool)
  - tensor(complex64)
  - tensor(complex128)
  description: Allow outputs to be any kind of tensor.
  type_param_str: T1
- allowed_type_strs:
  - tensor(float16)
  - tensor(float)
  - tensor(double)
  description: Allow inputs to be any kind of floating-point tensor.
  type_param_str: T2
