domain: ''
name: Selu
since_version: 6
min_input: 1
max_input: 1
min_output: 1
max_output: 1
doc: |-
  Selu takes one input data (Tensor<T>) and produces one output data
  (Tensor<T>) where the scaled exponential linear unit function,
  `y = gamma * (alpha * e^x - alpha) for x <= 0`, `y = gamma * x for x > 0`,
  is applied to the tensor elementwise.
attributes:
  - name: alpha
    description: >-
      Coefficient of SELU default to 1.67326319217681884765625 (i.e., float32 approximation
      of 1.6732632423543772848170429916717).
    type: FLOAT
    required: false
    default_value: 1.6732631921768188
  - name: gamma
    description: >-
      Coefficient of SELU default to 1.05070102214813232421875 (i.e., float32 approximation
      of 1.0507009873554804934193349852946).
    type: FLOAT
    required: false
    default_value: 1.0507010221481323
inputs:
  - name: X
    type_str: T
    description: Input tensor
    min_arity: 1
    tags:
      - differentiable
outputs:
  - name: Y
    type_str: T
    description: Output tensor
    min_arity: 1
    tags:
      - differentiable
type_constraints:
  - type_param_str: T
    description: Constrain input and output types to float tensors.
    allowed_type_strs:
      - tensor(float16)
      - tensor(float)
      - tensor(double)
function: |-
  <
    domain: "",
    opset_import: ["" : 18]
  >
  Selu <gamma,alpha>(X) => (Y)
  {
     Alpha = Constant <value_float: float = @alpha> ()
     AlphaCast = CastLike (Alpha, X)
     Gamma = Constant <value_float: float = @gamma> ()
     GammaCast = CastLike (Gamma, X)
     Zero = Constant <value: tensor = float {0}> ()
     ZeroCast = CastLike (Zero, X)
     ExpX = Exp (X)
     AlphaMulExpX = Mul (AlphaCast, ExpX)
     AlphaMulExpXSubAlpha = Sub (AlphaMulExpX, AlphaCast)
     Neg = Mul (GammaCast, AlphaMulExpXSubAlpha)
     Pos = Mul (GammaCast, X)
     XLessThanZero = Less (X, ZeroCast)
     Y = Where (XLessThanZero, Neg, Pos)
  }
support_level: COMMON
deprecated: false
