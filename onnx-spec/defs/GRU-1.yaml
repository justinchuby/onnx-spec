domain: ''
name: GRU
since_version: 1
min_input: 3
max_input: 6
min_output: 2
max_output: 2
doc: "\nComputes an one-layer GRU. This operator is usually supported via some custom\n\
  implementation such as CuDNN.\n\nNotations:\n\n`X` - input tensor\n\n`z` - update\
  \ gate\n\n`r` - reset gate\n\n`h` - hidden gate\n\n`t` - time step (t-1 means previous\
  \ time step)\n\n`W[zrh]` - W parameter weight matrix for update, reset, and hidden\
  \ gates\n\n`R[zrh]` - R recurrence weight matrix for update, reset, and hidden gates\n\
  \n`Wb[zrh]` - W bias vectors for update, reset, and hidden gates\n\n`Rb[zrh]` -\
  \ R bias vectors for update, reset, and hidden gates\n\n`WB[zrh]` - W parameter\
  \ weight matrix for backward update, reset, and hidden gates\n\n`RB[zrh]` - R recurrence\
  \ weight matrix for backward update, reset, and hidden gates\n\n`WBb[zrh]` - W bias\
  \ vectors for backward update, reset, and hidden gates\n\n`RBb[zrh]` - R bias vectors\
  \ for backward update, reset, and hidden gates\n\n`H` - Hidden state\n\n`num_directions`\
  \ - 2 if direction == bidirectional else 1\n\nActivation functions:\n\n  Relu(x)\
  \                - max(0, x)\n\n  Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})\n\
  \n  Sigmoid(x)             - 1/(1 + e^{-x})\n\n  (NOTE: Below are optional)\n\n\
  \  Affine(x)              - alpha*x + beta\n\n  LeakyRelu(x)           - x if x\
  \ >= 0 else alpha * x\n\n  ThresholdedRelu(x)     - x if x >= alpha else 0\n\n \
  \ ScaledTanh(x)          - alpha*Tanh(beta*x)\n\n  HardSigmoid(x)         - min(max(alpha*x\
  \ + beta, 0), 1)\n\n  Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)\n\
  \n  Softsign(x)            - x/(1 + |x|)\n\n  Softplus(x)            - log(1 + e^x)\n\
  \nEquations (Default: f=Sigmoid, g=Tanh):\n\n  - zt = f(Xt*(Wz^T) + Ht-1*Rz + Wbz\
  \ + Rbz)\n\n  - rt = f(Xt*(Wr^T) + Ht-1*Rr + Wbr + Rbr)\n\n  - ht = g(Xt*(Wh^T)\
  \ + (rt (.) Ht-1)*Rh + Rbh + Wbh) # default, when linear_before_reset = 0\n\n  -\
  \ ht = g(Xt*(Wh^T) + (rt (.) (Ht-1*Rh + Rbh) + Wbh) # when linear_before_reset !=\
  \ 0\n\n  - Ht = (1 - zt) (.) ht + zt (.) Ht-1\n"
attributes:
- name: activation_alpha
  description: Optional scaling values used by some activation functions. The values
    are consumed in the order of activation functions, for example (f, g, h) in LSTM.
  type: AttrType.FLOATS
  required: false
  default_value: null
- name: activation_beta
  description: Optional scaling values used by some activation functions. The values
    are consumed in the order of activation functions, for example (f, g, h) in LSTM.
  type: AttrType.FLOATS
  required: false
  default_value: null
- name: activations
  description: 'A list of 2 (or 4 if bidirectional) activation functions for update,
    reset, and hidden gates. The activation functions must be one of the activation
    functions specified above. Optional: See the equations for default if not specified.'
  type: AttrType.STRINGS
  required: false
  default_value: null
- name: clip
  description: Cell clip threshold. Clipping bounds the elements of a tensor in the
    range of [-threshold, +threshold] and is applied to the input of activations.
    No clip if not specified.
  type: AttrType.FLOAT
  required: false
  default_value: null
- name: direction
  description: Specify if the RNN is forward, reverse, or bidirectional. Must be one
    of forward (default), reverse, or bidirectional.
  type: AttrType.STRING
  required: false
  default_value: foward
- name: hidden_size
  description: Number of neurons in the hidden layer
  type: AttrType.INT
  required: false
  default_value: null
- name: output_sequence
  description: The sequence output for the hidden is optional if 0. Default 0.
  type: AttrType.INT
  required: false
  default_value: 0
inputs:
- name: X
  type_str: T
  description: The input sequences packed (and potentially padded) into one 3-D tensor
    with the shape of `[seq_length, batch_size, input_size]`.
  min_arity: 1
  tags: []
- name: W
  type_str: T
  description: The weight tensor for the gates. Concatenation of `W[zrh]` and `WB[zrh]`
    (if bidirectional) along dimension 0. This tensor has shape `[num_directions,
    3*hidden_size, input_size]`.
  min_arity: 1
  tags: []
- name: R
  type_str: T
  description: The recurrence weight tensor. Concatenation of `R[zrh]` and `RB[zrh]`
    (if bidirectional) along dimension 0. This tensor has shape `[num_directions,
    3*hidden_size, hidden_size]`.
  min_arity: 1
  tags: []
- name: B
  type_str: T
  description: 'The bias tensor for the gates. Concatenation of `[Wb[zrh], Rb[zrh]]`
    and `[WBb[zrh], RBb[zrh]]` (if bidirectional) along dimension 0. This tensor has
    shape `[num_directions, 6*hidden_size]`. Optional: If not specified - assumed
    to be 0'
  min_arity: 1
  tags:
  - optional
- name: sequence_lens
  type_str: T1
  description: Optional tensor specifying lengths of the sequences in a batch. If
    not specified - assumed all sequences in the batch to have length `seq_length`.
    It has shape `[batch_size]`.
  min_arity: 1
  tags:
  - optional
- name: initial_h
  type_str: T
  description: Optional initial value of the hidden. If not specified - assumed to
    be 0. It has shape `[num_directions, batch_size, hidden_size]`.
  min_arity: 1
  tags:
  - optional
outputs:
- name: Y
  type_str: T
  description: A tensor that concats all the intermediate output values of the hidden.
    It has shape `[seq_length, num_directions, batch_size, hidden_size]`. It is optional
    if `output_sequence` is 0.
  min_arity: 1
  tags:
  - optional
- name: Y_h
  type_str: T
  description: The last output value of the hidden. It has shape `[num_directions,
    batch_size, hidden_size]`.
  min_arity: 1
  tags: []
type_constraints:
- type_param_str: T
  description: Constrain input and output types to float tensors.
  allowed_type_strs:
  - tensor(float16)
  - tensor(float)
  - tensor(double)
- type_param_str: T1
  description: Constrain seq_lens to integer tensor.
  allowed_type_strs:
  - tensor(int32)
support_level: COMMON
deprecated: false
